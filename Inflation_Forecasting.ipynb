{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Libraries                                                                                                         #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200 # Plot resolution (dpi)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR # Import VAR class\n",
    "import shap\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# str_Dir_Plan_Data = '/Users/bodamjerry/Desktop/Oldenburg Studies/Winter2425/Applied Economics/data/PC/'\n",
    "\n",
    "str_Dir_Plan_FRED = '/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/FRED/'\n",
    "str_Dir_Plan_Data = '/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PCC/'\n",
    "str_Dir_Plan_PC = '/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PXNew/'\n",
    "\n",
    "str_Nome_Plan_FRED_MD = 'current'\n",
    "str_FRED_MD_Desc = 'Data_Description_MD.csv'\n",
    "\n",
    "X = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + '0 ' + 'X.csv')\n",
    "y = pd.read_csv(filepath_or_buffer = str_Dir_Plan_PC + '0 ' + 'y.csv')\n",
    "\n",
    "df_FRED_MD = pd.read_csv(filepath_or_buffer = str_Dir_Plan_FRED + str_Nome_Plan_FRED_MD + '.csv', skiprows=[1])\n",
    "\n",
    "# print(X.head())\n",
    "# print(y.head())\n",
    "df_date = X['Date']\n",
    "# data = X[['UNRATE', 'RETAILx', 'FEDFUNDS', 'CP3Mx', 'S&P 500']]\n",
    "data_t = pd.concat([X, y['CPIAUCSL']], axis=1)\n",
    "#\n",
    "data_t.set_index('Date', inplace=True)\n",
    "# data_t.drop('Date', axis=1, inplace=True)\n",
    "# data = data_t[['CPIAUCSL', 'UNRATE', 'HOUST', 'HOUSTNE', 'PERMITNE', 'AAA', 'BAA', 'PERMITMW', 'RETAILx', 'TB3MS', 'TB6MS', 'FEDFUNDS', 'CP3Mx', 'S&P 500']]\n",
    "data = data_t.copy()\n",
    "\n",
    "print(data.shape)\n",
    "print(data.columns)\n",
    "\n",
    "print(data.head())\n",
    "print(data.tail())\n",
    "\n",
    "\n",
    "# var_model.summary()"
   ],
   "id": "c3e98568ef57758"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def display_variable_descriptions(data_columns, description_file_path='Data_Description_MD.csv'):\n",
    "    \"\"\"\n",
    "    Displays the description and group of each variable in data_columns\n",
    "    by looking up information in the 'Data_Description_MD.csv' file.\n",
    "\n",
    "    Args:\n",
    "        data_columns (Index): pandas Index object containing column names.\n",
    "        description_file_path (str, optional): Path to the Data Description CSV file.\n",
    "                                                Defaults to 'Data_Description_MD.csv'.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        description_df = pd.read_csv(description_file_path, delimiter=';')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Description file not found at '{description_file_path}'.\")\n",
    "        return\n",
    "\n",
    "    print(\"Variable Descriptions and Groups:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for col_name in data_columns:\n",
    "        # Find the row in description_df where 'FRED' column matches the current column name\n",
    "        variable_info = description_df[description_df['FRED'] == col_name]\n",
    "\n",
    "        if not variable_info.empty:\n",
    "            description = variable_info['Description'].iloc[0]  # Get the description from the first row\n",
    "            group = variable_info['Group'].iloc[0]          # Get the group from the first row\n",
    "            print(f\"Variable: {col_name}\")\n",
    "            print(f\"  Description: {description}\")\n",
    "            print(f\"  Group: {group}\")\n",
    "        else:\n",
    "            print(f\"Variable: {col_name}\")\n",
    "            print(f\"  Description: Description not found in '{description_file_path}'\")\n",
    "            print(f\"  Group: Group not found in '{description_file_path}'\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "# Assuming you have 'data' DataFrame and 'data.columns' is available\n",
    "# and the 'Data_Description_MD.csv' file is in the same directory or you provide the correct path.\n",
    "\n",
    "# Example Usage (assuming 'data_l' is your loaded DataFrame):\n",
    "# After you run the code that loads your data and prints data.columns,\n",
    "# and you have the 'data' DataFrame available (e.g., data = data_l[all_variables].copy())\n",
    "# Uncomment and run the following lines:\n",
    "\n",
    "path = str_Dir_Plan_FRED + str_FRED_MD_Desc\n",
    "display_variable_descriptions(data.columns, path)"
   ],
   "id": "327641a6c7a0b0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def get_variable_descriptions(col_name, description_file_path='Data_Description_MD.csv'):\n",
    "\n",
    "    try:\n",
    "        description_df = pd.read_csv(description_file_path, delimiter=';')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Description file not found at '{description_file_path}'.\")\n",
    "        return\n",
    "\n",
    "    # print(\"Variable Descriptions and Groups:\")\n",
    "    # print(\"-\" * 40)\n",
    "\n",
    "    # Find the row in description_df where 'FRED' column matches the current column name\n",
    "    variable_info = description_df[description_df['FRED'] == col_name]\n",
    "\n",
    "    if not variable_info.empty:\n",
    "        description = variable_info['Description'].iloc[0]  # Get the description from the first row\n",
    "        group = variable_info['Group'].iloc[0]          # Get the group from the first row\n",
    "        return f\"{col_name}: {group}\"\n",
    "\n",
    "    else:\n",
    "        print(f\"Variable: {col_name}\")\n",
    "        # print(f\"  Description: Description not found in '{description_file_path}'\")\n",
    "        # print(f\"  Group: Group not found in '{description_file_path}'\")\n",
    "\n",
    "# # print(data.index)\n",
    "# for col_name in data.columns:\n",
    "#     plot_series(data[col_name], get_variable_descriptions(col_name, path))\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def plot_grid_series(data, column_names=None, grid_rows=4, grid_cols=3, figsize=(18, 14)):\n",
    "    \"\"\"\n",
    "    Plots time series for multiple columns in a DataFrame in a grid layout.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing time series data.\n",
    "        column_names (list, optional): List of column names to plot.\n",
    "                                        If None, all columns in data are plotted. Defaults to None.\n",
    "        grid_rows (int, optional): Number of rows in the subplot grid. Defaults to 4.\n",
    "        grid_cols (int, optional): Number of columns in the subplot grid. Defaults to 3.\n",
    "        figsize (tuple, optional): Figure size for the entire grid plot. Defaults to (18, 14).\n",
    "    \"\"\"\n",
    "\n",
    "    if column_names is None:\n",
    "        column_names = data.columns\n",
    "\n",
    "    num_plots = len(column_names)\n",
    "    fig, axes = plt.subplots(nrows=grid_rows, ncols=grid_cols, figsize=figsize)\n",
    "    axes = axes.flatten() # Flatten the 2D array of axes to easily iterate\n",
    "\n",
    "    for i, col_name in enumerate(column_names):\n",
    "        series = data[col_name]\n",
    "        ax = axes[i] # Get the current subplot axes\n",
    "\n",
    "        # --- Ensure series.index is a DateTimeIndex ---\n",
    "        if not isinstance(series.index, pd.DatetimeIndex):\n",
    "            try:\n",
    "                series.index = pd.to_datetime(series.index)\n",
    "            except ValueError:\n",
    "                print(f\"Warning: Series index for {col_name} not in DateTime format. Plotting without date formatting.\")\n",
    "                ax.plot(series.values, label=col_name, color='#2b8cbe', linestyle='-', linewidth=1.5)\n",
    "                ax.set_title(col_name, fontsize=14) # Smaller subplot title font\n",
    "                ax.set_xlabel('Time', fontsize=10)   # Smaller subplot xlabel font\n",
    "                ax.set_ylabel('Value', fontsize=10)  # Smaller subplot ylabel font\n",
    "                ax.tick_params(axis='x', rotation=45, labelsize=8) # Smaller tick label font\n",
    "                ax.grid(True, linestyle='--', alpha=0.5, color='gray')\n",
    "                ax.legend(fontsize='small', frameon=True, edgecolor='black')\n",
    "                continue # Skip date formatting for this subplot\n",
    "\n",
    "        ax.plot(series.index, series.values, label=col_name, color='#2b8cbe', linestyle='-', linewidth=1.5)\n",
    "\n",
    "        # --- Date Locator and Formatter for each subplot ---\n",
    "        locator = mdates.AutoDateLocator()\n",
    "        ax.xaxis.set_major_locator(locator)\n",
    "        formatter = mdates.DateFormatter('%Y-%m')\n",
    "        ax.xaxis.set_major_formatter(formatter)\n",
    "\n",
    "        ax.set_title(get_variable_descriptions(col_name, path), fontsize=14) # Smaller subplot title font\n",
    "        ax.set_xlabel('Date', fontsize=10)   # Smaller subplot xlabel font\n",
    "        ax.set_ylabel('Value', fontsize=10)  # Smaller subplot ylabel font\n",
    "\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=8) # Smaller tick label font\n",
    "        ax.grid(True, linestyle='--', alpha=0.5, color='gray')\n",
    "        ax.legend(fontsize='small', frameon=True, edgecolor='black')\n",
    "\n",
    "        # --- Customize Spines for each subplot ---\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_linewidth(0.5)\n",
    "        ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "\n",
    "    # Remove any unused subplots if num_plots is less than grid_rows * grid_cols\n",
    "    for j in range(num_plots, grid_rows * grid_cols):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97]) # Adjust layout for better spacing, make room for suptitle\n",
    "    plt.suptitle('Time Series Grid', fontsize=18) # Overall title for the grid\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assuming 'data' is your DataFrame with 12 columns\n",
    "plot_grid_series(data, grid_rows=3, grid_cols=4) # Plots all columns in a 4x3 grid (default)\n",
    "\n",
    "# If you want to plot only specific columns (e.g., first 6 columns in a 2x3 grid):\n",
    "# columns_to_plot = data.columns[:6]\n",
    "# plot_grid_series(data[columns_to_plot], column_names=columns_to_plot, grid_rows=2, grid_cols=3)\n"
   ],
   "id": "84419fadb4f24388"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_t = data.copy()\n",
    "def create_lag_features(df, columns, lags):\n",
    "    \"\"\"\n",
    "    Creates lag features for specified columns in a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to add lag features to.\n",
    "        columns (list): A list of column names for which to create lag features.\n",
    "        lags (int): The number of lags to create.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added lag features.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        for i in range(1, lags + 1):\n",
    "            df[f'{col}_L{i}'] = df[col].shift(i)\n",
    "    return df\n",
    "\n",
    "save_dir = \"/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PXShuffle\"\n",
    "\n",
    "# Uncomment to create random state\n",
    "\n",
    "# random.seed(123)\n",
    "# for i in range(100):\n",
    "#     random_state = random.randint(1, 1000)\n",
    "# \n",
    "#     # Shuffle data_t and save as CSV\n",
    "#     shuffled_data = data_t.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "#     shuffled_data.to_csv(os.path.join(save_dir, f\"shuffled_data_{i+1}.csv\"), index=False)\n",
    "# \n",
    "#     # Create data DataFrame with selected columns\n",
    "# \n",
    "# \n",
    "\n",
    "# data = data_t[['CPIAUCSL', 'UNRATE', 'RETAILx', 'FEDFUNDS', 'CP3Mx', 'S&P 500']]\n",
    "data.drop('HOUSTNE', axis=1, inplace=True)\n",
    "\n"
   ],
   "id": "a5d7505d38992222"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data.drop()\n",
    "\n",
    "# test_size = int(len(data) * 0.2)  # Convert percentage to absolute count\n",
    "test_size = 12\n",
    "train_df = data.iloc[:-test_size]\n",
    "test_df = data.iloc[-test_size:]\n",
    "\n",
    "print(test_df.shape)\n",
    "print(train_df.shape)\n",
    "\n",
    "print(data_t.shape)\n",
    "print(data.shape)"
   ],
   "id": "48bfb63f8226cd5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def check_stationarity(series, column_name):\n",
    "    \"\"\"Performs ADF test and prints results.\"\"\"\n",
    "    result = adfuller(series)\n",
    "    print(f'ADF Test Results for: {column_name}')\n",
    "    print('ADF Statistic: %f' % result[0])\n",
    "    print('p-value: %f' % result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print('\\t%s: %.3f' % (key, value))\n",
    "    if result[1] <= 0.05:\n",
    "        print(f\"{column_name} is likely stationary.\")\n",
    "    else:\n",
    "        print(f\"{column_name} is likely non-stationary.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Check stationarity for all columns\n",
    "for col in data.columns:\n",
    "    check_stationarity(data[col], col)\n",
    "\n",
    "# If any series are non-stationary, apply differencing\n",
    "data_diff = data.diff().dropna() # First-order differencing\n",
    "\n",
    "# Re-check stationarity after differencing\n",
    "print(\"\\nStationarity after Differencing:\")\n",
    "for col in data_diff.columns:\n",
    "    check_stationarity(data_diff[col], col)"
   ],
   "id": "80d8c150179f5959"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = VAR(train_df)\n",
    "\n",
    "# AIC based lag order (your original code)\n",
    "lag_order_aic = model.select_order(maxlags=15).aic\n",
    "print(f\"AIC-based Lag Order: {lag_order_aic}\")\n",
    "\n",
    "# BIC based lag order\n",
    "lag_order_bic = model.select_order(maxlags=15).bic\n",
    "print(f\"BIC-based Lag Order: {lag_order_bic}\")\n",
    "\n",
    "# HQIC based lag order\n",
    "lag_order_hqic = model.select_order(maxlags=15).hqic\n",
    "print(f\"HQIC-based Lag Order: {lag_order_hqic}\")\n",
    "\n",
    "# Choose the lag order based on the criteria you prefer or by comparing performance with different lag orders.\n",
    "lag_order = lag_order_aic # or lag_order_bic, or lag_order_hqic\n",
    "var_model = model.fit(lag_order)\n",
    "print(lag_order)\n",
    "\n",
    "var_model.summary()\n"
   ],
   "id": "fe5e942af81eb34e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Forecast the next 12 months\n",
    "\n",
    "forecast = var_model.forecast(train_df.values[-lag_order:], steps=test_size)\n",
    "# predict = var_model.get_prediction(start=train_df.shape[0], end=train_df.shape[0] + 11)\n",
    "forecast_df = pd.DataFrame(forecast, columns=train_df.columns)\n",
    "\n",
    "# Extract forecasted CPIAUCSL \n",
    "forecasted_cpiauscl = forecast_df['CPIAUCSL']\n",
    "\n",
    "actual_cpiauscl = test_df['CPIAUCSL']\n",
    "\n",
    "mae = mean_absolute_error(actual_cpiauscl, forecasted_cpiauscl)\n",
    "rmse = np.sqrt(mean_squared_error(actual_cpiauscl, forecasted_cpiauscl))\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\") # Apply seaborn visual theme\n",
    "\n",
    "def plot_forecast(forecasted_series, actual_series, title_name='Forecasted vs Actual CPIAUCSL'):\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6)) # Store figure object\n",
    "\n",
    "    actual_series.index = pd.to_datetime(actual_series.index) # Try converting to DateTimeIndex\n",
    "\n",
    "    plt.plot(actual_series.index, forecasted_series, label='Forecasted', color='#e41a1c', linestyle='dashed', linewidth=2) # Red for forecast\n",
    "    plt.plot(actual_series.index, actual_series, label='Actual', color='#377eb8', linestyle='solid', linewidth=2) # Blue for actual\n",
    "\n",
    "    plt.xlabel('Date', fontsize=10)\n",
    "    plt.ylabel('Value', fontsize=10)\n",
    "    plt.title(title_name, fontsize=12)\n",
    "\n",
    "    # Date formatting for x-axis (using logic from your improved plot_series)\n",
    "    ax = plt.gca()\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    formatter = mdates.DateFormatter('%Y-%m')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10) # Rotate x-axis labels, adjust font size\n",
    "    plt.gcf().autofmt_xdate() # Auto-format dates\n",
    "\n",
    "    plt.legend(fontsize=10, frameon=True, shadow=False, edgecolor='black')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "    # Customize Spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "    plt.tight_layout() # Adjust layout\n",
    "\n",
    "    return fig # Return the figure object (do NOT use plt.show() here if you want to control display later)\n",
    "\n",
    "\n",
    "def plot_observed_vs_fitted(var_model_fit, train_df, target_column='CPIAUCSL', title='VAR Model - Observed vs Fitted (Training)'):\n",
    "\n",
    "    fitted_values = var_model_fit.fittedvalues # Get fitted values from the VAR model\n",
    "    observed_train = train_df[target_column]   # Observed values from the training data\n",
    "\n",
    "    # Extract fitted values for the target column\n",
    "    fitted_cpiauscl = fitted_values[target_column]\n",
    "\n",
    "    # Ensure index is DateTimeIndex if possible (align with actual_series index if needed)\n",
    "    try:\n",
    "        fitted_cpiauscl.index = pd.to_datetime(fitted_cpiauscl.index)\n",
    "        observed_train.index = pd.to_datetime(observed_train.index) # Align observed index too for plotting\n",
    "        if not fitted_cpiauscl.index.equals(observed_train.index): # If indexes still not the same, try to align based on actual index\n",
    "            fitted_cpiauscl.index = observed_train.index # Force index alignment for plotting if possible\n",
    "    except ValueError:\n",
    "        print(\"Warning: Could not convert index to DateTime for Observed vs Fitted plot.\")\n",
    "        pass # Plot without DateTimeIndex if conversion fails\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.plot(observed_train.index, observed_train, label=\"Observed (Train)\", linestyle=\"solid\", linewidth=2) # Blue for observed\n",
    "    plt.plot(fitted_cpiauscl.index, fitted_cpiauscl, label=\"Fitted (Train)\", linestyle=\"solid\", linewidth=2) # Green for fitted\n",
    "\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.xlabel(\"Date\", fontsize=10)\n",
    "    plt.ylabel(\"Value\", fontsize=10)\n",
    "    plt.legend(fontsize=10, frameon=True, shadow=False, edgecolor='black')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "    # Date formatting for x-axis\n",
    "    ax = plt.gca()\n",
    "    locator = mdates.AutoDateLocator() # Use observed_train index for locator\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    formatter = mdates.DateFormatter('%Y-%m')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    # Customize Spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "# Generate the plot by calling the function:\n",
    "var_model_forecast_plot = plot_forecast(forecasted_cpiauscl, actual_cpiauscl, title_name='VAR Model Forecast - CPIAUCSL')\n",
    "var_observed_vs_fitted_plot = plot_observed_vs_fitted(var_model, train_df, target_column='CPIAUCSL', title='VAR Model - Observed vs Fitted (Training)')\n",
    "\n",
    "# To display the plot immediately (if you are not planning to put it in a grid right now):\n",
    "plt.show() # Call plt.show() after plot_forecast if you want to display it now\n",
    "\n",
    "# --- Later, if you want to put this plot in a grid ---\n",
    "# You would collect these figure objects from different models (ARIMA, SARIMA, CNN-LSTM etc.)\n",
    "# and then use them in your plot_grid_series function (or a modified version) to arrange them in a grid.\n",
    "\n"
   ],
   "id": "550624983f4806f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# 1. ARIMA Model for CPIAUCSL\n",
    "\n",
    "# Extract CPIAUCSL series for ARIMA\n",
    "train_cpiauscl_arima = train_df['CPIAUCSL']\n",
    "test_cpiauscl_arima = test_df['CPIAUCSL']"
   ],
   "id": "5f839f45390c85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For ARIMA order selection (non-seasonal):\n",
    "auto_arima_model_arima = auto_arima(train_cpiauscl_arima, seasonal=False, stepwise=True,\n",
    "                                    suppress_warnings=True, error_action=\"ignore\", max_order=10, trace=True)\n",
    "print(auto_arima_model_arima.order) # Get the recommended (p, d, q) order\n",
    "\n"
   ],
   "id": "8fdb0742893d76ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For SARIMA order selection (seasonal):\n",
    "auto_arima_model_sarima = auto_arima(train_cpiauscl_arima, seasonal=True, m=12, stepwise=True, # m=12 for monthly seasonality\n",
    "                                     suppress_warnings=True, error_action=\"ignore\", max_order=10, trace=True,\n",
    "                                     max_p=5, max_q=5, max_P=5, max_Q=5) # Limit search space\n",
    "print(auto_arima_model_sarima.order) # Get recommended (p, d, q)\n",
    "print(auto_arima_model_sarima.seasonal_order) # Get recommended (P, D, Q, s)"
   ],
   "id": "e88dcd5176a04b76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def plot_arima_observed_vs_fitted(model_fit, train_data, target_column='CPIAUCSL', title='VAR Model - Observed vs Fitted (Training)'):\n",
    "\n",
    "\n",
    "    fitted_series = model_fit.fittedvalues # For ARIMA, fittedvalues is already a Series for the fitted series\n",
    "\n",
    "    observed_train = train_data[target_column] if isinstance(train_data, pd.DataFrame) else train_data # Handle DataFrame or Series input for train_data\n",
    "\n",
    "    # Extract fitted values for the target column\n",
    "    # fitted_cpiauscl = fitted_values[target_column]\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Ensure index is DateTimeIndex if possible (align with actual_series index if needed)\n",
    "    plt.plot(observed_train.index, observed_train, label=\"Observed (Train)\", linestyle=\"solid\", linewidth=2) # Blue for observed\n",
    "    plt.plot(fitted_series.index, fitted_series, label=\"Fitted (Train)\", linestyle=\"solid\", linewidth=2) # Green for fitted\n",
    "\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.xlabel(\"Date\", fontsize=10)\n",
    "    plt.ylabel(\"Value\", fontsize=10)\n",
    "    plt.legend(fontsize=10, frameon=True, shadow=False, edgecolor='black')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7, color='gray')\n",
    "\n",
    "    # Date formatting for x-axis\n",
    "    ax = plt.gca()\n",
    "    locator = mdates.AutoDateLocator() # Use observed_train index for locator\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    formatter = mdates.DateFormatter('%Y-%m')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    # Customize Spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n"
   ],
   "id": "e2365fd98c16c97a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Assuming 'data' DataFrame is already loaded and preprocessed\n",
    "# and train_df, test_df are already created as in your VAR code\n",
    "\n",
    "# 1. ARIMA Model for CPIAUCSL\n",
    "\n",
    "# ARIMA Model Order (p, d, q) - You'll need to determine this based on ACF, PACF, or auto_arima\n",
    "# For demonstration, let's assume a simple order (you should optimize this)\n",
    "# arima_order = (5, 1, 0) # Example order - Autoregressive order (p=5), Integrated (d=1), Moving Average (q=0)\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = ARIMA(train_cpiauscl_arima, order=auto_arima_model_arima.order)\n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Forecast for test period (24 steps)\n",
    "arima_forecast = arima_model_fit.forecast(steps=test_size)\n",
    "\n",
    "# Evaluate ARIMA model\n",
    "mae_arima = mean_absolute_error(test_cpiauscl_arima, arima_forecast)\n",
    "rmse_arima = np.sqrt(mean_squared_error(test_cpiauscl_arima, arima_forecast))\n",
    "\n",
    "print(f'ARIMA Model - Mean Absolute Error (MAE): {mae_arima:.4f}')\n",
    "print(f'ARIMA Model - Root Mean Squared Error (RMSE): {rmse_arima:.4f}')\n",
    "\n",
    "# Plot ARIMA Forecast\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(test_cpiauscl_arima.index, arima_forecast, label='ARIMA Forecasted CPIAUCSL', color='blue')\n",
    "# plt.plot(test_cpiauscl_arima.index, test_cpiauscl_arima, label='Actual CPIAUCSL', color='red', linestyle='--')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('CPIAUCSL')\n",
    "# plt.title('ARIMA Forecasted vs Actual CPIAUCSL')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "arima_forecast = plot_forecast(arima_forecast, actual_cpiauscl, title='ARIMA: Forecasted vs Actual')\n",
    "arima_fitted = plot_arima_observed_vs_fitted(arima_model_fit, train_df, title='ARIMA: Observed vs Fitted')\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "9f50a630a5cd16c2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# 2. SARIMA Model for CPIAUCSL\n",
    "\n",
    "# Extract CPIAUCSL series (already done above: train_cpiauscl_arima, test_cpiauscl_arima)\n",
    "\n",
    "# SARIMA Model Order (p, d, q) and Seasonal Order (P, D, Q, s)\n",
    "# You'll need to determine these based on ACF, PACF, seasonal decomposition, or auto_arima\n",
    "# For demonstration, let's assume simple orders (you should optimize this)\n",
    "# sarima_order = (5, 1, 0)          # (p, d, q) - Non-seasonal order - same as ARIMA for simplicity in example\n",
    "# sarima_seasonal_order = (1, 1, 0, 12) # (P, D, Q, s) - Seasonal order, s=12 for yearly seasonality (monthly data)\n",
    "\n",
    "# Fit SARIMA model\n",
    "sarima_model = SARIMAX(train_cpiauscl_arima,\n",
    "                       order=auto_arima_model_sarima.order,\n",
    "                       seasonal_order=auto_arima_model_sarima.seasonal_order)\n",
    "sarima_model_fit = sarima_model.fit(disp=False) # disp=False to suppress convergence output\n",
    "\n",
    "# Forecast for test period (24 steps)\n",
    "sarima_forecast = sarima_model_fit.forecast(steps=test_size)\n",
    "\n",
    "# Evaluate SARIMA model\n",
    "mae_sarima = mean_absolute_error(test_cpiauscl_arima, sarima_forecast)\n",
    "rmse_sarima = np.sqrt(mean_squared_error(test_cpiauscl_arima, sarima_forecast))\n",
    "\n",
    "\n",
    "sarima_forecast = plot_forecast(sarima_forecast, actual_cpiauscl)\n",
    "sarima_fitted = plot_arima_observed_vs_fitted(sarima_model_fit, train_df)\n",
    "plt.show()\n",
    "\n",
    "print(f'SARIMA Model - Mean Absolute Error (MAE): {mae_sarima:.4f}')\n",
    "print(f'SARIMA Model - Root Mean Squared Error (RMSE): {rmse_sarima:.4f}')\n",
    "\n",
    "# model = VAR(train_df)\n",
    "# var_model = model.fit(lag_order)\n",
    "# forecast = var_model.forecast(train_df.values[-lag_order:], steps=12)\n",
    "# forecast_df = pd.DataFrame(forecast, columns=train_df.columns)\n",
    "# \n",
    "# forecasted_cpiauscl = forecast_df['CPIAUCSL']\n",
    "# actual_cpiauscl = test_df['CPIAUCSL']\n"
   ],
   "id": "129b1257e041eaef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for serial correlation in residuals\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "dw_test_results = durbin_watson(var_model.resid)\n",
    "print(\"\\nDurbin-Watson Test for Residual Autocorrelation:\")\n",
    "for col, dw in zip(train_df.columns, dw_test_results):\n",
    "    print(f\"{col}: DW Statistic = {dw:.4f} (Ideal: ~2)\")"
   ],
   "id": "5ff165fa35458ad0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Directory to save CSV files\n",
    "save_dir = \"/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PXShuffle\"\n",
    "\n",
    "def train_and_evaluate_var_model(data, lag_order):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a VAR model on a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with lagged time series data.\n",
    "        lag_order (int): The lag order for the VAR model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: MAE, RMSE, and forecasted CPIAUCSL values.\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = data.iloc[:-12]\n",
    "    test_df = data.iloc[-12:]\n",
    "\n",
    "    model = VAR(train_df)\n",
    "    var_model = model.fit(lag_order)\n",
    "    forecast = var_model.forecast(train_df.values[-lag_order:], steps=12)\n",
    "    forecast_df = pd.DataFrame(forecast, columns=train_df.columns)\n",
    "\n",
    "    forecasted_cpiauscl = forecast_df['CPIAUCSL']\n",
    "    actual_cpiauscl = test_df['CPIAUCSL']\n",
    "\n",
    "    mae = mean_absolute_error(actual_cpiauscl, forecasted_cpiauscl)\n",
    "    rmse = np.sqrt(mean_squared_error(actual_cpiauscl, forecasted_cpiauscl))\n",
    "\n",
    "    return mae, rmse, forecasted_cpiauscl\n",
    "\n",
    "# Perform robust testing with 50 random states\n",
    "var_maes = []\n",
    "var_rmses = []\n",
    "var_all_forecasts = []\n",
    "\n",
    "for i in range(100):\n",
    "    # Read shuffled data from CSV file\n",
    "    file_path = os.path.join(save_dir, f\"shuffled_data_{i+1}.csv\")\n",
    "    shuffled_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Create data DataFrame with selected columns\n",
    "    # data = shuffled_data[['CPIAUCSL_1', 'UNRATE', 'RETAILx', 'FEDFUNDS', 'CP3Mx', 'S&P 500']]\n",
    "    data = shuffled_data.copy()\n",
    "    mae, rmse, forecasted_cpiauscl = train_and_evaluate_var_model(data, 9)\n",
    "    var_maes.append(mae)\n",
    "    var_rmses.append(rmse)\n",
    "    var_all_forecasts.append(forecasted_cpiauscl)\n",
    "\n",
    "# ... (rest of your code for calculating averages and plotting)"
   ],
   "id": "aac7d9af94ce2f41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Calculate and print average MAE and RMSE for the VAR model\n",
    "avg_var_mae = np.mean(var_maes)\n",
    "avg_var_rmse = np.mean(var_rmses)\n",
    "\n",
    "print(f'VAR Average MAE: {avg_var_mae}')\n",
    "print(f'VAR Average RMSE: {avg_var_rmse}')\n",
    "\n",
    "# Calculate standard deviation for MAE and RMSE\n",
    "std_var_mae = np.std(var_maes)\n",
    "std_var_rmse = np.std(var_rmses)\n",
    "\n",
    "# Plotting the MAEs and RMSEs with shaded confidence intervals\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot VAR MAEs with shaded confidence interval\n",
    "plt.plot(range(1, 101), var_maes, label='VAR MAE', marker='o', linestyle='-', color='blue', alpha=0.8)\n",
    "plt.fill_between(range(1, 101),\n",
    "                 np.array(var_maes) - std_var_mae,\n",
    "                 np.array(var_maes) + std_var_mae,\n",
    "                 color='blue', alpha=0.2, label='MAE Confidence Interval')\n",
    "\n",
    "# Add horizontal lines for average MAE and RMSE\n",
    "plt.axhline(y=avg_var_mae, color='blue', linestyle='--', alpha=0.5, label=f'Avg VAR MAE: {avg_var_mae:.2f}')\n",
    "# plt.axhline(y=avg_var_rmse, color='green', linestyle='--', alpha=0.5, label=f'Avg VAR RMSE: {avg_var_rmse:.2f}')\n",
    "\n",
    "# Add annotations for average values\n",
    "plt.text(101, avg_var_mae, f'Avg MAE: {avg_var_mae:.2f}', color='blue', va='center', ha='left')\n",
    "# plt.text(101, avg_var_rmse, f'Avg RMSE: {avg_var_rmse:.2f}', color='green', va='center', ha='left')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('VAR: MAE and RMSE over 100 Random States with Confidence Intervals', fontsize=16)\n",
    "plt.xlabel('Random State', fontsize=14)\n",
    "plt.ylabel('Error', fontsize=14)\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fb1c41fed1f07dbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_l = data.copy()\n",
    "# data_l = data_l.drop('CPIAUCSL', axis=1)\n",
    "\n",
    "# print(data_t.head());\n",
    "print(data_l.head());\n",
    "# print(data_l.tail());"
   ],
   "id": "2a95d68f8189c048"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "columns_to_lag = ['CPIAUCSL_1', 'UNRATE', 'RETAILx', 'FEDFUNDS', 'S&P 500', 'CP3Mx']\n",
    "# data_ls = create_lag_features(data_l, columns_to_lag, lags=5)\n",
    "# data_ls.dropna(inplace=True)\n",
    "print(data_l.head())\n",
    "\n",
    "# print(data_ls.head())"
   ],
   "id": "282cac2e7f0f7622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def prepare_data_for_cnn_lstm(data_l, target_col='CPIAUCSL', forecast_horizon=1, lags=3, test_size=0.2):\n",
    "\n",
    "    # Copy original data to avoid modifying in place\n",
    "    data_x = data_l.copy()\n",
    "    # print(data_x.head())\n",
    "\n",
    "    # Initialize scalers\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    # Separate target and features\n",
    "    y = data_x[[target_col]]\n",
    "    X = data_x.drop(target_col, axis=1)\n",
    "\n",
    "    # Scale the features and target\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    # Convert back to DataFrame (preserving column names and index)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    y_scaled_df = pd.DataFrame(y_scaled, columns=y.columns, index=y.index)\n",
    "\n",
    "    # Concatenate scaled features and target\n",
    "    data = pd.concat([X_scaled_df, y_scaled_df], axis=1)\n",
    "\n",
    "    # Shift the target column to align with forecasting\n",
    "    data[target_col+ '1'] = data_x[target_col]\n",
    "    data[target_col] = data_x[target_col].shift(-forecast_horizon)\n",
    "\n",
    "    columns_to_lag = [col for col in data.columns if col != target_col] # All columns except target\n",
    "    # Create lagged features\n",
    "    data_ls = create_lag_features(data, columns_to_lag, lags=lags)\n",
    "    data_ls.dropna(inplace=True)\n",
    "    return data_ls, scaler_X, scaler_y\n",
    "\n",
    "# Example usage\n",
    "# X_train, y_train, X_test, y_test, scaler_X, scaler_y, feature_c = prepare_data_for_cnn_lstm(data_l, 'CPIAUCSL')\n",
    "data_ls, scaler_X, scaler_y = prepare_data_for_cnn_lstm(data_l, 'CPIAUCSL')\n",
    "# print(data_ls.head())\n",
    "# print(data_ls.columns)\n",
    "print(data_ls.shape)"
   ],
   "id": "7fbc06959e9df9c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def prepare_data(data, target_column='CPIAUCSL', test_size=12, test_size_percent=False):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets and reshapes it for ConvLSTM2D.\n",
    "\n",
    "    Parameters:\n",
    "    - data (DataFrame): The input dataset.\n",
    "    - target_column (str): The column name of the target variable.\n",
    "    - test_size (int or float): Number of test samples (if int) or percentage of data for testing (if float).\n",
    "    - test_size_percent (bool): If True, interprets test_size as a percentage (0-1).\n",
    "\n",
    "    Returns:\n",
    "    - X_train, y_train, X_test, y_test: Training and testing datasets, reshaped for ConvLSTM2D.\n",
    "    \"\"\"\n",
    "    if test_size_percent:\n",
    "        test_size = int(len(data) * test_size)  # Convert percentage to absolute count\n",
    "\n",
    "    print(data.head())\n",
    "\n",
    "    train_df = data.iloc[:-test_size]\n",
    "    test_df = data.iloc[-test_size:]\n",
    "    train_index = train_df.index\n",
    "    test_index = test_df.index\n",
    "\n",
    "    X_train = train_df.drop(target_column, axis=1).values\n",
    "    y_train = train_df[target_column].values\n",
    "    X_test = test_df.drop(target_column, axis=1).values\n",
    "    y_test = test_df[target_column].values\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, 1, 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, 1, 1, X_test.shape[1]))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, train_index, test_index\n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"Builds and compiles a ConvLSTM2D model.\"\"\"\n",
    "    model = Sequential([\n",
    "        Bidirectional(ConvLSTM2D(filters=128, kernel_size=(1, 1), activation='selu', return_sequences=True, input_shape=input_shape)),\n",
    "        Bidirectional(ConvLSTM2D(filters=64, kernel_size=(1, 1), activation='selu', return_sequences=False)),\n",
    "        Flatten(),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    opt = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train):\n",
    "    \"\"\"Trains the model with early stopping.\"\"\"\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, epochs=300, batch_size=288, validation_split=0.2, callbacks=[early_stopping])\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the model and returns the test loss and predictions.\"\"\"\n",
    "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    predictions = model.predict(X_test, verbose=0)\n",
    "    return loss, predictions\n",
    "\n",
    "def inverse_transform(scaler, data):\n",
    "    \"\"\"Inverse transforms the scaled data.\"\"\"\n",
    "    return scaler.inverse_transform(data.reshape(-1, 1))\n",
    "\n",
    "def plot_predictions(y_actual, y_pred, train_index, title='CNN-LSTM: Actual vs Forecasted Values'):\n",
    "    \"\"\"Plots actual vs forecasted values.\"\"\"\n",
    "    train_index = pd.to_datetime(train_index)\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_index, y_actual, label='Actual', color='blue', linestyle='solid')\n",
    "    plt.plot(train_index, y_pred, label='Forecasted', color='red', linestyle='dashed')\n",
    "\n",
    "    ax = plt.gca()\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    formatter = mdates.DateFormatter('%Y-%m')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(title, fontsize=10)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def plot_loss(history):\n",
    "    # Plots loss function\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('LSTM - Loss (MSE)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_observed_vs_fitted(y_train, predictions_train, train_index):\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    train_index = pd.to_datetime(train_index)\n",
    "    plt.plot(train_index, y_train, label=\"Observed (Train)\", linestyle=\"solid\")\n",
    "    plt.plot(train_index, predictions_train, label=\"Fitted (Train)\", linestyle=\"solid\")\n",
    "    plt.title(\"CNN-LSTM - Observed vs. Fitted Values\")\n",
    "\n",
    "    ax = plt.gca()\n",
    "    locator = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(locator)\n",
    "    formatter = mdates.DateFormatter('%Y-%m')\n",
    "    ax.xaxis.set_major_formatter(formatter)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "def main(data, scaler_y):\n",
    "    \"\"\"Main function to execute the pipeline.\"\"\"\n",
    "    X_train, y_train, X_test, y_test, train_index, test_index = prepare_data(data, test_size=12, test_size_percent=False)\n",
    "    model = build_model((1, 1, 1, X_train.shape[-1]))\n",
    "    history = train_model(model, X_train, y_train)\n",
    "\n",
    "    loss, predictions = evaluate_model(model, X_test, y_test)\n",
    "    print(f'Test Loss: {loss}')\n",
    "\n",
    "    y_test_actual = inverse_transform(scaler_y, y_test)\n",
    "    predictions_actual = inverse_transform(scaler_y, predictions)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_actual, predictions_actual)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual, predictions_actual))\n",
    "    plot_cnn_forecast = plot_predictions(y_test_actual, predictions_actual, test_index)\n",
    "    # plot_loss(history)\n",
    "\n",
    "    predictions_train = model.predict(X_train, verbose=0) # Predict on X_train\n",
    "    predictions_train_actual = inverse_transform(scaler_y, predictions_train) # Inverse transform fitted values\n",
    "    y_train_actual = inverse_transform(scaler_y, y_train) # Inverse transform y_train\n",
    "\n",
    "    # --- Plot Observed vs Fitted on Training Data ---\n",
    "    plot_cnn_obs_vs_fitted = plot_observed_vs_fitted(y_train_actual, predictions_train_actual, train_index) # Pass y_train_actual and predictions_train_actual\n",
    "\n",
    "    # plot_observed_vs_fitted(y_train, predictions)\n",
    "\n",
    "    print(f'Mean Absolute Error (MAE): {mae}')\n",
    "    print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "    return plot_cnn_forecast, plot_cnn_obs_vs_fitted, model, X_train, X_test\n",
    "\n",
    "# Call main(data_lst, scaler_y) with your actual data and scaler\n",
    "# plot_cnn_forecast, plot_cnn_obs_vs_fitted, model, X_train, X_test = main(data_ls.copy(), scaler_y)\n",
    "prepare_data(data_ls[['CPIAUCSL1', 'CPIAUCSL', 'CPIAUCSL1_L1', 'CPIAUCSL1_L2', 'CPIAUCSL1_L3']])\n"
   ],
   "id": "61ec7fb5bdc150ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Wrapper function for SHAP\n",
    "def model_predict(X):\n",
    "    \"\"\"Predict function for SHAP that ensures correct input shape.\"\"\"\n",
    "    X = X.reshape((X.shape[0], 1, 1, 1, X.shape[1]))  # Reshape to original model input shape\n",
    "    return model.predict(X, verbose=0).flatten()  # Flatten output for SHAP\n",
    "\n",
    "# Select a subset of X_test for SHAP analysis (SHAP can be slow)\n",
    "feature_names = data_ls.drop(columns=['CPIAUCSL1']).columns.tolist()\n",
    "num_samples = min(50, X_test.shape[0])  # Ensure we don't exceed the available samples\n",
    "\n",
    "X_sample = X_test[:num_samples].reshape((num_samples, X_test.shape[-1]))  # Convert to 2D\n",
    "X_sample_df = pd.DataFrame(X_sample, columns=feature_names)  # Convert NumPy array back to DataFrame\n",
    "# All Sample\n",
    "# X_sample_df = pd.DataFrame(X_test.reshape(X_test.shape[0], X_test.shape[-1]), columns=feature_names) \n",
    "\n",
    "\n",
    "# Create Kernel SHAP Explainer\n",
    "explainer = shap.KernelExplainer(model_predict, X_sample_df)\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer.shap_values(X_sample_df)\n",
    "\n",
    "# Create SHAP summary plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Adjust figure size if needed\n",
    "shap.summary_plot(shap_values, X_sample_df, show=False)  # Prevent immediate display\n",
    "\n",
    "# Adjust font sizes\n",
    "for item in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "    item.set_fontsize(8)  # Change this value to your desired font size\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot summary plot\n",
    "# shap.summary_plot(shap_values, X_sample_df)\n"
   ],
   "id": "e5851cf6ba0afc07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def plot_grid_series(figures, titles, grid_rows=2, grid_cols=4, figsize=(20, 10)):\n",
    "    \"\"\"\n",
    "    Plots a grid of time series plots.\n",
    "\n",
    "    Args:\n",
    "        figures (list of plt.Figure): List of matplotlib Figure objects to be plotted in the grid.\n",
    "        titles (list of str): List of titles for each subplot, corresponding to the figures.\n",
    "        grid_rows (int): Number of rows in the grid.\n",
    "        grid_cols (int): Number of columns in the grid.\n",
    "        figsize (tuple): Figure size for the entire grid plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows=grid_rows, ncols=grid_cols, figsize=figsize)\n",
    "    axes = axes.flatten() # Flatten the 2D array of axes for easy iteration\n",
    "\n",
    "    for i, fig_obj in enumerate(figures):\n",
    "        ax = fig_obj.axes[0] # Assuming each figure has one axes (common for these plots)\n",
    "        target_ax = axes[i]\n",
    "\n",
    "        # Copy lines from the original axes to the subplot\n",
    "        for line in ax.lines:\n",
    "            target_ax.plot(line.get_xdata(), line.get_ydata(),\n",
    "                           color=line.get_color(), linestyle=line.get_linestyle(),\n",
    "                           label=line.get_label(), linewidth=line.get_linewidth())\n",
    "\n",
    "        # Copy title, labels, legend from the original axes\n",
    "        target_ax.set_title(titles[i])\n",
    "        target_ax.set_xlabel(ax.get_xlabel())\n",
    "        target_ax.set_ylabel(ax.get_ylabel())\n",
    "\n",
    "        # Handle legend - only if there are lines with labels\n",
    "        if ax.get_legend() is not None:\n",
    "            target_ax.legend()\n",
    "\n",
    "        target_ax.grid(True, linestyle='--', alpha=0.7, color='gray') # Ensure grid is on in subplots\n",
    "\n",
    "        # Date formatting - try to apply if x-axis is date-like\n",
    "        try:\n",
    "            target_ax.xaxis.set_major_locator(ax.xaxis.get_major_locator())\n",
    "            target_ax.xaxis.set_major_formatter(ax.xaxis.get_major_formatter())\n",
    "            for label in target_ax.get_xticklabels():\n",
    "                label.set_ha(\"right\")\n",
    "                label.set_rotation(45)\n",
    "        except AttributeError:\n",
    "            pass # If no date formatting, ignore error\n",
    "\n",
    "        # Customize Spines - copy spine settings if needed (optional, can be customized for grid)\n",
    "        target_ax.spines['top'].set_visible(False)\n",
    "        target_ax.spines['right'].set_visible(False)\n",
    "        target_ax.spines['bottom'].set_linewidth(0.5)\n",
    "        target_ax.spines['left'].set_linewidth(0.5)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Example Usage: ---\n",
    "plot_figures = [\n",
    "    var_model_forecast_plot,\n",
    "    var_observed_vs_fitted_plot,\n",
    "    arima_forecast,\n",
    "    arima_fitted,\n",
    "\n",
    "    sarima_forecast,\n",
    "    sarima_fitted,\n",
    "    plot_cnn_forecast,\n",
    "    plot_cnn_obs_vs_fitted\n",
    "]\n",
    "\n",
    "plot_titles = [\n",
    "    'VAR - Forecast',\n",
    "    'VAR - Observed vs Fitted',\n",
    "    'ARIMA - Forecast',\n",
    "    'ARIMA - Fitted',\n",
    "    'SARIMA - Forecast',\n",
    "    'SARIMA - Fitted',\n",
    "    'CNN-LSTM - Forecast',\n",
    "    'CNN-LSTM - Observed vs Fitted'\n",
    "]\n",
    "\n",
    "plot_grid_series(plot_figures, plot_titles)\n",
    "\n"
   ],
   "id": "706078c6334d477d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " def prep_data_for_cnn(data_l, target_col='CPIAUCSL', forecast_horizon=1, lags=3, test_size=12, test_size_percent=False):\n",
    "\n",
    "    # Copy original data to avoid modifying in place\n",
    "    data_x = data_l.copy()\n",
    "    # print(data_x.head())\n",
    "\n",
    "    # Initialize scalers\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    # Separate target and features\n",
    "    y = data_x[[target_col]]\n",
    "    X = data_x.drop(target_col, axis=1)\n",
    "\n",
    "    # Scale the features and target\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    # Convert back to DataFrame (preserving column names and index)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "    y_scaled_df = pd.DataFrame(y_scaled, columns=y.columns, index=y.index)\n",
    "\n",
    "    # Concatenate scaled features and target\n",
    "    data = pd.concat([X_scaled_df, y_scaled_df], axis=1)\n",
    "\n",
    "    # Shift the target column to align with forecasting\n",
    "    data[target_col+ '1'] = data_x[target_col]\n",
    "    data[target_col] = data_x[target_col].shift(-forecast_horizon)\n",
    "\n",
    "    columns_to_lag = [col for col in data.columns if col != target_col] # All columns except target\n",
    "\n",
    "    # Create lagged features\n",
    "    data_ls = create_lag_features(data, columns_to_lag, lags=lags)\n",
    "\n",
    "    # Drop NaN values caused by shifting\n",
    "    data_ls.dropna(inplace=True)\n",
    "\n",
    "    if test_size_percent:\n",
    "        test_size = int(len(data) * test_size)  # Convert percentage to absolute count\n",
    "\n",
    "    train_df = data.iloc[:-test_size]\n",
    "    test_df = data.iloc[-test_size:]\n",
    "\n",
    "    X_train = train_df.drop(target_col, axis=1).values\n",
    "    y_train = train_df[target_col].values\n",
    "    X_test = test_df.drop(target_col, axis=1).values\n",
    "    y_test = test_df[target_col].values\n",
    "\n",
    "    X_train = X_train.reshape((X_train.shape[0], 1, 1, 1, X_train.shape[1]))\n",
    "    X_test = X_test.reshape((X_test.shape[0], 1, 1, 1, X_test.shape[1]))\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scaler_y\n",
    "    # return data_ls, scaler_X, scaler_y\n"
   ],
   "id": "a0c3447f96e95f6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Perform robust testing with 50 random states\n",
    "maes = []\n",
    "rmses = []\n",
    "cnn_all_predictions = []\n",
    "\n",
    "save_dir = \"/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PXShuffle\"\n",
    "\n",
    "for i in range(100):\n",
    "    file_path = os.path.join(save_dir, f\"shuffled_data_{i+1}.csv\")\n",
    "    shuffled_data = pd.read_csv(file_path)\n",
    "\n",
    "    X_train, y_train, X_test, y_test, scaler_y = prep_data_for_cnn(shuffled_data, 'CPIAUCSL', test_size=0.2, test_size_percent=True)\n",
    "    model = build_model((1, 1, 1, X_train.shape[-1]))\n",
    "    loss, predictions = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    y_test_actual = inverse_transform(scaler_y, y_test)\n",
    "    predictions_actual = inverse_transform(scaler_y, predictions)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_actual, predictions_actual)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_actual, predictions_actual))\n",
    "\n",
    "    # main(data, scaler_y)\n",
    "    maes.append(mae)\n",
    "    rmses.append(rmse)\n",
    "    cnn_all_predictions.append(predictions)\n",
    "\n",
    "# Calculate and print average MAE and RMSE\n",
    "avg_mae = np.mean(maes)\n",
    "avg_rmse = np.mean(rmses)\n",
    "\n",
    "# Plotting the MAEs and RMSEs\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot VAR MAEs\n",
    "plt.plot(range(1, 101), maes, label='LSTM MAE', marker='o', linestyle='-', color='blue')\n",
    "\n",
    "# Plot VAR RMSEs\n",
    "plt.plot(range(1, 101), rmses, label='LSTM RMSE', marker='s', linestyle='-', color='green')\n",
    "\n",
    "plt.title('LSTM: MAE and RMSE over 50 Random States')\n",
    "plt.xlabel('Random State')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average RMSE: {avg_rmse}')\n"
   ],
   "id": "5e90d61f4cc62c30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Fit SARIMA model\n",
    "\n",
    "# Directory to save CSV files\n",
    "save_dir = \"/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PXShuffle\"\n",
    "\n",
    "def train_and_evaluate_sarima_model(data, auto_arima_model_sarima,  test_size = 12, test_size_percent=False):\n",
    "\n",
    "    if test_size_percent:\n",
    "        test_size = int(len(data) * test_size)  # Convert percentage to absolute count\n",
    "\n",
    "    train_df = data.iloc[:-test_size]\n",
    "    test_df = data.iloc[-test_size:]\n",
    "    # 1. ARIMA Model for CPIAUCSL\n",
    "\n",
    "    # Extract CPIAUCSL series for ARIMA\n",
    "    train_cpiauscl_arima = train_df['CPIAUCSL']\n",
    "    test_cpiauscl_arima = test_df['CPIAUCSL']\n",
    "\n",
    "    sarima_model = SARIMAX(train_cpiauscl_arima,\n",
    "                           order=auto_arima_model_sarima.order,\n",
    "                           seasonal_order=auto_arima_model_sarima.seasonal_order)\n",
    "\n",
    "    sarima_model_fit = sarima_model.fit(disp=False) # disp=False to suppress convergence output\n",
    "\n",
    "    # Forecast for test period (24 steps)\n",
    "    sarima_forecast = sarima_model_fit.forecast(steps=test_size)\n",
    "\n",
    "    # Evaluate SARIMA model\n",
    "    mae_sarima = mean_absolute_error(test_cpiauscl_arima, sarima_forecast)\n",
    "    rmse_sarima = np.sqrt(mean_squared_error(test_cpiauscl_arima, sarima_forecast))\n",
    "\n",
    "    # model = VAR(train_df)\n",
    "    # var_model = model.fit(lag_order)\n",
    "    # forecast = var_model.forecast(train_df.values[-lag_order:], steps=12)\n",
    "    # forecast_df = pd.DataFrame(forecast, columns=train_df.columns)\n",
    "    # \n",
    "    # forecasted_cpiauscl = forecast_df['CPIAUCSL']\n",
    "    # actual_cpiauscl = test_df['CPIAUCSL']\n",
    "\n",
    "    mae = mean_absolute_error(actual_cpiauscl, forecasted_cpiauscl)\n",
    "    rmse = np.sqrt(mean_squared_error(actual_cpiauscl, forecasted_cpiauscl))\n",
    "\n",
    "    return mae_sarima, rmse_sarima, sarima_forecast\n",
    "\n",
    "# Perform robust testing with 50 random states\n",
    "var_maes = []\n",
    "var_rmses = []\n",
    "var_all_forecasts = []\n",
    "\n",
    "for i in range(100):\n",
    "    # Read shuffled data from CSV file\n",
    "    file_path = os.path.join(save_dir, f\"shuffled_data_{i+1}.csv\")\n",
    "    shuffled_data = pd.read_csv(file_path)\n",
    "\n",
    "    # Create data DataFrame with selected columns\n",
    "    # data = shuffled_data[['CPIAUCSL_1', 'UNRATE', 'RETAILx', 'FEDFUNDS', 'CP3Mx', 'S&P 500']]\n",
    "    data = shuffled_data.copy()\n",
    "\n",
    "    mae, rmse, forecasted_cpiauscl = train_and_evaluate_sarima_model(data, auto_arima_model_sarima, test_size = 0.2, test_size_percent=True)\n",
    "    var_maes.append(mae)\n",
    "    var_rmses.append(rmse)\n",
    "    var_all_forecasts.append(forecasted_cpiauscl)\n",
    "\n",
    "\n",
    "\n",
    "# ... (rest of your code for calculating averages and plotting)"
   ],
   "id": "e16bfd1fbba3f49c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "avg_sarima_mae = np.mean(var_maes)\n",
    "avg_sarima_rmse = np.mean(var_rmses)\n",
    "\n",
    "print(f'Average MAE: {avg_sarima_mae}')\n",
    "print(f'Average RMSE: {avg_sarima_rmse}')\n"
   ],
   "id": "9bf59dde07183028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compare performance at each step\n",
    "conv_lstm_step_errors = np.array(cnn_all_predictions).reshape(100, 12)  # Reshape to (100, 12)\n",
    "var_step_errors = np.array(var_all_forecasts).reshape(100, 12)\n",
    "\n",
    "# Calculate mean absolute error for each step\n",
    "conv_lstm_step_mae = np.mean(np.abs(conv_lstm_step_errors), axis=0)  # Average across 100 runs\n",
    "var_step_mae = np.mean(np.abs(var_step_errors), axis=0)\n",
    "\n",
    "# Compare MAE at each step\n",
    "better_model_steps = [\"ConvLSTM\" if c_mae < v_mae else \"SARIMA\" for c_mae, v_mae in zip(conv_lstm_step_mae, var_step_mae)]\n",
    "\n",
    "# Print the results\n",
    "# Print the results with values\n",
    "for i, (c_mae, v_mae) in enumerate(zip(conv_lstm_step_mae, var_step_mae)):\n",
    "    better_model = \"ConvLSTM\" if c_mae < v_mae else \"SARIMA\"\n",
    "    print(f\"Step {i+1}: {better_model} performs better (ConvLSTM MAE: {c_mae:.4f}, SARIMA MAE: {v_mae:.4f})\")"
   ],
   "id": "9dd26a52fe810f8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Calculate standard deviation for each step\n",
    "conv_lstm_step_std = np.std(np.abs(conv_lstm_step_errors), axis=0)  # Standard deviation across 100 runs\n",
    "var_step_std = np.std(np.abs(var_step_errors), axis=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot ConvLSTM MAE with shaded confidence interval\n",
    "plt.plot(range(1, 13), conv_lstm_step_mae, label='ConvLSTM MAE', marker='o', linestyle='-', color='blue', alpha=0.8)\n",
    "plt.fill_between(range(1, 13),\n",
    "                 conv_lstm_step_mae - conv_lstm_step_std,\n",
    "                 conv_lstm_step_mae + conv_lstm_step_std,\n",
    "                 color='blue', alpha=0.2, label='ConvLSTM Confidence Interval')\n",
    "\n",
    "# Plot VAR MAE with shaded confidence interval\n",
    "plt.plot(range(1, 13), var_step_mae, label='VAR MAE', marker='x', linestyle='--', color='red', alpha=0.8)\n",
    "plt.fill_between(range(1, 13),\n",
    "                 var_step_mae - var_step_std,\n",
    "                 var_step_mae + var_step_std,\n",
    "                 color='red', alpha=0.2, label='SARIMA Confidence Interval')\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('ConvLSTM vs SARIMA: Average MAE with Confidence Intervals over 12 Forecasting Steps', fontsize=16)\n",
    "plt.xlabel('Forecasting Step', fontsize=14)\n",
    "plt.ylabel('Average MAE', fontsize=14)\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "cca0c02204eb33f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ... (your existing code)\n",
    "\n",
    "# Calculate root mean squared error for each step\n",
    "conv_lstm_step_rmse = np.sqrt(np.mean(conv_lstm_step_errors**2, axis=0))  # RMSE calculation\n",
    "var_step_rmse = np.sqrt(np.mean(var_step_errors**2, axis=0))\n",
    "\n",
    "# Compare RMSE at each step\n",
    "better_model_steps = [\"ConvLSTM\" if c_rmse < v_rmse else \"SARIMA\" for c_rmse, v_rmse in zip(conv_lstm_step_rmse, var_step_rmse)]\n",
    "\n",
    "# Print the results with values\n",
    "for i, (c_rmse, v_rmse) in enumerate(zip(conv_lstm_step_rmse, var_step_rmse)):\n",
    "    better_model = \"ConvLSTM\" if c_rmse < v_rmse else \"SARIMA\"\n",
    "    print(f\"Step {i+1}: {better_model} performs better (ConvLSTM RMSE: {c_rmse:.4f}, SARIMA: {v_rmse:.4f})\")"
   ],
   "id": "1b1be938b0ee0c15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "# conv_lstm_step_rmse = np.sqrt(np.mean(conv_lstm_step_errors**2, axis=0))  # RMSE calculation\n",
    "# var_step_rmse = np.sqrt(np.mean(var_step_errors**2, axis=0))\n",
    "\n",
    "# Plot ConvLSTM and VAR `RMSEs'\n",
    "plt.plot(range(1, 13), conv_lstm_step_rmse, label='ConvLSTM RMSE', marker='o', linestyle='-', color='blue', alpha=0.8)\n",
    "plt.plot(range(1, 13), var_step_rmse, label='SARIMA RMSE', marker='x', linestyle='--', color='red', alpha=0.8)\n",
    "\n",
    "plt.title('ConvLSTM vs SARIMA: Average RMSE over 12 Forecasting Steps')\n",
    "plt.xlabel('Forecasting Step')\n",
    "plt.ylabel('Average RMSE')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "9443500538d7f340"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
