{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-19T11:42:25.882669Z",
     "start_time": "2025-02-19T11:42:21.800323Z"
    }
   },
   "source": [
    "# R Path - required by rpy2 prior to importing libraries\n",
    "import os\n",
    "import os\n",
    "os.environ[\"R_HOME\"] = \"/Library/Frameworks/R.framework/Resources\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/Library/Frameworks/R.framework/Resources/lib/\"\n",
    "\n",
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Data\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "# R\n",
    "from rpy2.robjects import pandas2ri\n",
    "\n",
    "# Kalman Filter\n",
    "from pykalman import KalmanFilter\n",
    "\n",
    "# Machine Learning - tensorflow, keras, and sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import Dense, Flatten, Lambda, MaxPooling3D, Conv3D, RepeatVector, Layer\n",
    "\n",
    "# SKLearn Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Statsmodels\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Univariate GARCH\n",
    "\n",
    "# Plots\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Misc\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_selection import RFE, mutual_info_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Constants and Parameters                                                                                          #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Folders and worksheet names\n",
    "str_Dir_Plan_FRED = '/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/FRED/'\n",
    "str_Dir_Plan_Data = '/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PX/'\n",
    "str_Dir_Plan_PC = '/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/PXNew/'\n",
    "str_Dir_Results = '/Users/bodamjerry/Desktop/Winter2425/Applied Economics/data/Results/'\n",
    "\n",
    "str_Nome_Plan_FRED_MD = 'current'\n",
    "# str_Nome_Plan_FRED_MD = '2015-01'\n",
    "str_Nome_Plan_FRED_MD_Desc = 'Data_Description_MD'\n",
    "\n",
    "# How to display plots\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.dpi'] = 200 # Plot resolution (dpi)\n",
    "\n",
    "# Required to convert datatypes from Python to R and vice-versa\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Remove warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Color style (plots)\n",
    "sns.set(color_codes = True)\n",
    "\n",
    "# Statistical significance for hypothesis testing\n",
    "# Using 1% due to the high number of tests carried out\n",
    "alfa = 0.01\n",
    "\n",
    "# Test size (share of observations used to build the test sample)\n",
    "share_test_size = 0.20\n",
    "\n",
    "# Validation sample size (share of observations used to build the validation sample)\n",
    "share_validation_size = 0.20\n",
    "\n",
    "# Number of lags considered when splitting the data - see LSTM models\n",
    "n_lags_lstm = 2\n",
    "\n",
    "# Number of lags considered when splitting the data - see ConvLSTM models\n",
    "n_lags_conv = 2\n",
    "\n",
    "# Number of sequences into which sample are broken when fitting ConvLSTM\n",
    "# Note: n_lags = n_seq * n_steps\n",
    "n_seq_conv = 1\n",
    "\n",
    "# Size of each sequence into which sample are broken when fitting ConvLSTM\n",
    "# Note: n_lags = n_seq * n_steps\n",
    "n_steps_conv = int(n_lags_conv / n_seq_conv)\n",
    "\n",
    "# Activation function\n",
    "act_fun = 'selu'\n"
   ],
   "id": "fdf9422f5a882a4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Auxiliary Functions                                                                                               #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "# Split a univariate sequence into samples\n",
    "def split_sequence_uni(sequence, n_steps, per_ahead, cum = False):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix + per_ahead - 1 > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        if cum == False:\n",
    "            seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + per_ahead - 1]\n",
    "        else:\n",
    "            seq_x, seq_y = sequence[i:end_ix], np.sum(sequence[end_ix:(end_ix + per_ahead)])\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Split a multivariate sequence into samples\n",
    "def split_sequence_mult(sequences, n_steps, per_ahead, cum = False):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix + per_ahead - 1 > len(sequences)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        if cum == False:\n",
    "            seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix + per_ahead - 1, -1]\n",
    "        else:\n",
    "            seq_x, seq_y = sequences[i:end_ix, :-1], np.sum(sequences[end_ix:(end_ix + per_ahead), -1])\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Kalman filter regression\n",
    "# If EM = True, then EM algorithm is used for estimation\n",
    "# delta is related to the variance of the betas. Delta -> 1 makes betas more volatile, which may lead to overfitting.\n",
    "# However, delta -> 0 may increase the MSE.\n",
    "\n",
    "def KFReg(X, y, delta, obs_cov, init_mean, init_cov, EM = False):\n",
    "    n_features = X.shape[1]\n",
    "    obs_mat = X[:, np.newaxis, :]\n",
    "    if EM == False:\n",
    "        trans_cov = (delta/(1 - delta))*np.eye(n_features)\n",
    "        kf = KalmanFilter(n_dim_obs = 1, n_dim_state = n_features,\n",
    "                          initial_state_mean = init_mean,\n",
    "                          initial_state_covariance = init_cov,\n",
    "                          transition_matrices = np.eye(n_features),\n",
    "                          observation_matrices = obs_mat,\n",
    "                          observation_covariance = obs_cov,\n",
    "                          transition_covariance = trans_cov)\n",
    "        state_means, state_covs = kf.filter(y)\n",
    "    else:\n",
    "        kf = KalmanFilter(n_dim_obs = 1, n_dim_state = n_features,\n",
    "                          initial_state_mean = init_mean,\n",
    "                          initial_state_covariance = init_cov,\n",
    "                          observation_matrices = obs_mat)\n",
    "    state_means, state_covs = kf.em(y).filter(y)\n",
    "    return state_means, state_covs, kf\n",
    "\n",
    "# Mean Absolute Error\n",
    "def MAE(y_obs, y_hat):\n",
    "    return np.mean(np.abs(y_obs - y_hat))\n",
    "\n",
    "# Mean Squared Error\n",
    "def MSE(y_obs, y_hat):\n",
    "    return np.mean((y_obs - y_hat)**2)\n",
    "\n",
    "# RMSE\n",
    "def RMSE(y_obs, y_hat):\n",
    "    return np.sqrt(MSE(y_obs, y_hat))\n",
    "\n",
    "def MAPE(y_obs, y_hat):\n",
    "    return np.mean(np.abs(y_obs - y_hat)/y_obs)\n",
    "\n",
    "def cos_sim(y_obs, y_hat):\n",
    "    return np.dot(y_obs, y_hat)/(np.linalg.norm(y_obs)*np.linalg.norm(y_hat))\n",
    "\n",
    "def R2(y_obs, y_hat):\n",
    "    SSR = np.sum((y_obs - y_hat)**2)\n",
    "    SST = np.sum((y_obs - np.mean(y_obs))**2)\n",
    "    return (1 - SSR/SST)\n",
    "\n",
    "# Variational autoencoder\n",
    "# Use those parameters to sample new points from the latent space:\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "# z_mean = []\n",
    "# z_log_var = []\n",
    "\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# Variational autoencoder\n",
    "\n",
    "def vae(X_train, X_test, intermediate_dim, latent_dim, batch_size, epochs, verbose, plot_name):\n",
    "\n",
    "    original_dim = X_train.shape[1]\n",
    "    input_shape = (original_dim, )\n",
    "\n",
    "    # Map inputs to the latent distribution parameters:\n",
    "    # VAE model = encoder + decoder\n",
    "    # build encoder model\n",
    "    inputs = Input(shape=input_shape, name='encoder_input')\n",
    "\n",
    "    print(\"Encoder Input Shape:\", inputs.shape)\n",
    "\n",
    "    x = Dense(intermediate_dim, activation='relu', name='intermediate_encoding')(inputs)\n",
    "    print(\"Intermediate Layer Shape:\", x.shape)\n",
    "\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "    print(\"z_mean Shape:\", z_mean.shape)\n",
    "    print(\"z_log_var Shape:\", z_log_var.shape)\n",
    "\n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "    # z = SamplingLayer(name='z')([z_mean, z_log_var])\n",
    "    print(\"Sampling Layer Output Shape:\", z.shape)\n",
    "\n",
    "    # Instantiate the encoder model:\n",
    "    # encoder = Model(inputs, z_mean)\n",
    "    encoder = Model(inputs=inputs, outputs=[z_mean, z_log_var, z], name='encoder')\n",
    "    #encoder = Model(inputs=inputs, outputs=z, name='encoder')\n",
    "    encoder.summary()\n",
    "\n",
    "    # Build the decoder model:\n",
    "    print(\"LATENT DIMENSION:\", latent_dim)\n",
    "    latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "    print(\"Decoder Input Shape:\", latent_inputs.shape)\n",
    "\n",
    "    x = Dense(intermediate_dim, activation='relu', name='intermediate_decoding')(latent_inputs)\n",
    "    outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "    print(\"Decoder Output Shape:\", outputs.shape)\n",
    "\n",
    "    # Instantiate the decoder model:\n",
    "    decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "    decoder.summary()\n",
    "    if latent_dim == 1:\n",
    "        print(\"GOT HERE 1\")\n",
    "\n",
    "    # Instantiate the VAE model:\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "\n",
    "    # outputs = decoder(encoder(inputs))\n",
    "    vae = Model(inputs, outputs, name='vae_mlp')\n",
    "    if latent_dim == 1:\n",
    "        print(\"GOT HERE 2\")\n",
    "\n",
    "\n",
    "    # Define the VAE loss function\n",
    "    def vae_loss(inputs, outputs, z_mean, z_log_var, original_dim):\n",
    "        bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        reconstruction_loss = bce_loss(inputs, outputs) * original_dim\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        return K.mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "\n",
    "    # Add the loss to the model\n",
    "    # vae.add_loss(vae_loss(inputs, outputs, z_mean, z_log_var, original_dim))\n",
    "    # Compile the model\n",
    "    if latent_dim == 1:\n",
    "        print(\"GOT HERE 3\")\n",
    "    loss_output = Lambda(lambda x: vae_loss(*x, original_dim))([inputs, outputs, z_mean, z_log_var])\n",
    "    vae = Model(inputs=inputs, outputs=[outputs, loss_output])  # Include loss in the outputs\n",
    "    vae.compile(optimizer='rmsprop', loss=['mse', None])  # Use 'mse' for the first output (reconstruction), None for the second (loss)\n",
    "\n",
    "    # vae.compile(optimizer='rmsprop', loss=None)\n",
    "    if latent_dim == 1:\n",
    "        print(\"GOT HERE 4\")\n",
    "    # Finally, we train the model:\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    results = vae.fit(X_train, X_train,\n",
    "                      shuffle=True,\n",
    "                      epochs=epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      validation_split=0.1,\n",
    "                      verbose=verbose)\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "\n",
    "    '''\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(results.history['loss'])\n",
    "    plt.plot(results.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.savefig(plot_name + '_loss')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Use the encoded layer to encode the training input\n",
    "    if latent_dim == 1:\n",
    "        print(\"GOT HERE 5\")\n",
    "    encoded_data = encoder.predict(X_train)[2]\n",
    "    print(encoded_data)\n",
    "    encoded_data = pd.DataFrame(data = encoded_data,\n",
    "                                index = X_train.index,\n",
    "                                columns = [\"PC_vae\" + str(i) for i in np.arange(0, latent_dim)])\n",
    "\n",
    "    '''\n",
    "    # Plots encoded data\n",
    "    sns.lineplot(data = encoded_data)\n",
    "    plt.savefig(plot_name + '_PC')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Correlation matrix\n",
    "    print(\"Encoded data Correlation Matrix\")\n",
    "    print(encoded_data.corr())\n",
    "\n",
    "    # Encoded data\n",
    "    if latent_dim == 1:\n",
    "        print(\"GOT HERE 5\")\n",
    "    X_train_encoded_vae = encoded_data\n",
    "    encoded_data = encoder.predict(X_test)[2]\n",
    "    encoded_data = pd.DataFrame(data = encoded_data,\n",
    "                                index = X_test.index,\n",
    "                                columns = [\"PC_vae\" + str(i) for i in np.arange(0, latent_dim)])\n",
    "    X_test_encoded_vae = encoded_data\n",
    "\n",
    "    X_train_encoded_vae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_train.csv')\n",
    "    X_test_encoded_vae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_test.csv')\n",
    "    if latent_dim == 1:\n",
    "        print(\"GOT HERE 6\")\n",
    "\n",
    "    return X_train_encoded_vae, X_test_encoded_vae\n",
    "\n",
    "def deep_ae(X_train, X_test, intermediate_dim, latent_dim, batch_size, epochs, verbose, plot_name):\n",
    "\n",
    "    # Number of time series\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    # if np.isnan(X_train).any() or np.isinf(X_train).any():\n",
    "    #     print(\"Warning: X_train contains NaNs or Infs. Please check your data.\")\n",
    "    # if np.isnan(X_test).any() or np.isinf(X_test).any():\n",
    "    #     print(\"Warning: X_test contains NaNs or Infs. Please check your data.\")\n",
    "    # \n",
    "    # Dimension of encoding units (roughly equivalent to principal components)\n",
    "    encoding_dim1 = intermediate_dim\n",
    "    encoding_dim2 = latent_dim\n",
    "\n",
    "    # Autoencoder architecture\n",
    "    input_img = Input(shape=(input_dim,), name = 'encoder_input')\n",
    "    encoded_partial = Dense(encoding_dim1, activation = \"selu\", name = 'intermediate_encoding')(input_img)\n",
    "    encoded = Dense(encoding_dim2, activation=\"selu\", name = 'encoding_layer')(encoded_partial)\n",
    "    decoded_partial = Dense(encoding_dim1, activation=\"selu\", name = 'intermediate_decoding')(encoded)\n",
    "    decoded = Dense(input_dim, activation=\"selu\", name = 'decoding_layer')(decoded_partial)\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    print(autoencoder.summary())\n",
    "\n",
    "    # Fits the autoencoder\n",
    "    hist_autoencoder = autoencoder.fit(X_train, X_train,\n",
    "                                       epochs=epochs,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=True,\n",
    "                                       validation_split=0.1,\n",
    "                                       verbose=verbose)\n",
    "\n",
    "    # Use the encoded layer to encode the training input\n",
    "    encoder = Model(input_img, encoded)\n",
    "    encoded_input = Input(shape=(encoding_dim1,))\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    encoded_data = encoder.predict(X_train)\n",
    "\n",
    "    '''\n",
    "    # Plots loss function\n",
    "    plt.plot(hist_autoencoder.history['loss'])\n",
    "    plt.plot(hist_autoencoder.history['val_loss'])\n",
    "    plt.title('Model Train vs. Validation Loss (MSE)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    plt.savefig(plot_name + '_loss')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Converts encoded data to a labeled dataframe\n",
    "    encoded_data = pd.DataFrame(data = encoded_data,\n",
    "                                index = X_train.index,\n",
    "                                columns = [\"PC_ae\" + str(i) for i in np.arange(0, encoding_dim2)])\n",
    "\n",
    "    '''\n",
    "    # Plots encoded data\n",
    "    sns.lineplot(data = encoded_data)\n",
    "    plt.savefig(plot_name + '_PC')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Correlation matrix\n",
    "    encoded_data.corr()\n",
    "\n",
    "    # Stores the encoded data\n",
    "    X_train_encoded_ae = encoded_data\n",
    "    encoded_data = encoder.predict(X_test)\n",
    "    encoded_data = pd.DataFrame(data = encoded_data,\n",
    "                                index = X_test.index,\n",
    "                                columns = [\"PC_ae\" + str(i) for i in np.arange(0, encoding_dim2)])\n",
    "    X_test_encoded_ae = encoded_data\n",
    "\n",
    "    X_train_encoded_ae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_train.csv')\n",
    "    X_test_encoded_ae.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_test.csv')\n",
    "\n",
    "    return X_train_encoded_ae, X_test_encoded_ae\n",
    "\n",
    "def pca_decomp(X_train, X_test, threshold, plot_name):\n",
    "\n",
    "    # Runs PCA for the maximum number of components possible\n",
    "    n_series = X_train.shape[1]\n",
    "    pca = PCA(n_components = n_series, svd_solver = 'full')\n",
    "    pca.fit(X_train)\n",
    "\n",
    "    # Selects the number of PCs required for explained variance > threshold\n",
    "    total_var = 0\n",
    "    n_comp = 0\n",
    "    for var in pca.explained_variance_ratio_:\n",
    "        total_var = var + total_var\n",
    "        n_comp = n_comp + 1\n",
    "        if total_var > threshold:\n",
    "            break\n",
    "\n",
    "    # Runs PCA for the number of components selected\n",
    "    pca = PCA(n_components = n_comp, svd_solver = 'full')\n",
    "    pca.fit(X_train)\n",
    "\n",
    "    # Applies transformation to training data\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_train_pca = X_train_pca.reshape(X_train_pca.shape[0], n_comp)\n",
    "    X_train_pca = pd.DataFrame(data = X_train_pca,\n",
    "                               index = X_train.index,\n",
    "                               columns = [\"PC\" + str(i) for i in np.arange(0,n_comp)])\n",
    "\n",
    "    # Applies transformation to test data\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    X_test_pca = X_test_pca.reshape(X_test_pca.shape[0], n_comp)\n",
    "    X_test_pca = pd.DataFrame(data = X_test_pca,\n",
    "                              index = X_test.index,\n",
    "                              columns = [\"PC\" + str(i) for i in np.arange(0,n_comp)])\n",
    "\n",
    "    '''\n",
    "    # Plots training data\n",
    "    sns.lineplot(data = X_train_pca)\n",
    "    plt.savefig(plot_name + '_PC')\n",
    "    plt.close()\n",
    "    '''\n",
    "\n",
    "    # Correlation matrix\n",
    "    X_train_pca.corr()\n",
    "\n",
    "    # Saves results\n",
    "    X_train_pca.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_train.csv')\n",
    "    X_test_pca.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + plot_name + '_test.csv')\n",
    "\n",
    "    return X_train_pca, X_test_pca\n",
    "\n"
   ],
   "id": "941f43085e292705"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#####################################################################################################################\n",
    "#                                                                                                                   #\n",
    "# Data                                                                                                              #\n",
    "#                                                                                                                   #\n",
    "#####################################################################################################################\n",
    "\n",
    "df_FRED_MD = pd.read_csv(filepath_or_buffer = str_Dir_Plan_FRED + str_Nome_Plan_FRED_MD + '.csv', skiprows=[1])\n",
    "# df_FRED_MD.index = pd.to_datetime(df_FRED_MD.iloc[:,0])\n",
    "df_date = df_FRED_MD['Date']\n",
    "\n",
    "# df_FRED_MD = df_FRED_MD.drop(columns = 'Date')\n",
    "df_FRED_MD['Date'] = pd.to_datetime(df_FRED_MD['Date'], format='%m/%d/%y', errors='coerce')\n",
    "# Convert years from 2059-2099 to 1959-1999\n",
    "df_FRED_MD.loc[df_FRED_MD['Date'].dt.year > 2024, 'Date'] -= pd.DateOffset(years=100)\n",
    "df_FRED_MD.set_index('Date', inplace=True)\n",
    "\n",
    "# print(df_FRED_MD.dropna())\n",
    "\n",
    "print(df_FRED_MD.head(5))\n",
    "print(df_FRED_MD.tail(5))\n",
    "\n",
    "df_FRED_Desc_MD = pd.read_csv(filepath_or_buffer = str_Dir_Plan_FRED + str_Nome_Plan_FRED_MD_Desc + '.csv', sep = ';')\n",
    "df_FRED_Desc_MD = df_FRED_Desc_MD.drop(columns = 'Index')\n",
    "\n"
   ],
   "id": "3cb5ca15846ab7dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "def plot_correlation_matrices(df_desc, df_data, group_column, series_column):\n",
    "    \"\"\"\n",
    "    Plots correlation matrices for each group of time series data.\n",
    "\n",
    "    Args:\n",
    "      df_desc: DataFrame containing series descriptions and group names.\n",
    "      df_data: DataFrame containing the time series data.\n",
    "      group_column: Name of the column with group labels in df_desc.\n",
    "      series_column: Name of the column with series names in df_desc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get unique group names\n",
    "    group_names = df_desc[group_column].unique()\n",
    "\n",
    "    for group_name in group_names:\n",
    "        # Filter series for the current group\n",
    "        series_names = df_desc[df_desc[group_column] == group_name][series_column]\n",
    "\n",
    "        try:\n",
    "            # Select corresponding data from df_data\n",
    "            group_data = df_data[series_names]\n",
    "\n",
    "            # Calculate the correlation matrix\n",
    "            corr_matrix = group_data.corr()\n",
    "\n",
    "            # Plot the correlation matrix\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "            plt.title(f'Correlation Matrix - {group_name}')\n",
    "            plt.show()\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Series not found - {e}\")\n",
    "\n",
    "\n",
    "# Assuming your DataFrames are named df_FRED_Desc_MD and df_FRED_MD\n",
    "plot_correlation_matrices(df_FRED_Desc_MD, df_FRED_MD, 'Group', 'FRED')\n",
    "\n",
    "\n",
    "def select_highly_correlated_variables(df_desc, df_data, group_column, series_column, corr_threshold):\n",
    "    \"\"\"\n",
    "    Selects highly correlated variables within each group of time series data.\n",
    "    Handles KeyError if a series is not found in the data.\n",
    "\n",
    "    Args:\n",
    "      df_desc: DataFrame containing series descriptions and group names.\n",
    "      df_data: DataFrame containing the time series data.\n",
    "      group_column: Name of the column with group labels in df_desc.\n",
    "      series_column: Name of the column with series names in df_desc.\n",
    "      corr_threshold: Absolute correlation threshold for selecting variables.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary where keys are group names and values are lists of \n",
    "      highly correlated variable pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    highly_correlated_vars = {}\n",
    "    group_names = df_desc[group_column].unique()\n",
    "\n",
    "    for group_name in group_names:\n",
    "        highly_correlated_pairs = []  # Initialize here\n",
    "\n",
    "        try:\n",
    "            series_names = df_desc[df_desc[group_column] == group_name][series_column]\n",
    "            group_data = df_data[series_names]\n",
    "            corr_matrix = group_data.corr()\n",
    "\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                    var1 = corr_matrix.columns[i]\n",
    "                    var2 = corr_matrix.columns[j]\n",
    "                    corr_value = abs(corr_matrix.iloc[i, j])\n",
    "                    if corr_value >= corr_threshold:\n",
    "                        highly_correlated_pairs.append((var1, var2, corr_value))\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Series not found in group {group_name} - {e}\")\n",
    "\n",
    "        highly_correlated_vars[group_name] = highly_correlated_pairs  # Assign outside the try block\n",
    "\n",
    "    return highly_correlated_vars\n",
    "\n",
    "\n",
    "# Assuming your DataFrames are named df_FRED_Desc_MD and df_FRED_MD\n",
    "highly_correlated_vars = select_highly_correlated_variables(\n",
    "    df_FRED_Desc_MD, df_FRED_MD, 'Group', 'FRED', corr_threshold=0.8\n",
    ")\n",
    "\n",
    "# Print the highly correlated variables for each group\n",
    "for group_name, pairs in highly_correlated_vars.items():\n",
    "    print(f\"\\nGroup: {group_name}\")\n",
    "    if pairs:\n",
    "        for var1, var2, corr_value in pairs:\n",
    "            print(f\"  - {var1} and {var2} (Correlation: {corr_value:.2f})\")\n",
    "    else:\n",
    "        print(\"  - No highly correlated variables found.\")\n"
   ],
   "id": "e8d87b14f5cc50ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the list of series in df_FRED_Desc_MD\n",
    "listed_series = set(df_FRED_Desc_MD['FRED'])\n",
    "\n",
    "# Get the list of columns in df_FRED_MD\n",
    "all_columns = set(df_FRED_MD.columns)\n",
    "\n",
    "# Find columns in df_FRED_MD that are not listed in df_FRED_Desc_MD\n",
    "unmatched_columns = all_columns - listed_series\n",
    "# Display the result\n",
    "print(\"Columns in df_FRED_MD that do not belong to any group:\")\n",
    "print(unmatched_columns)"
   ],
   "id": "1f5bfab8abf660a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def correlation_filter(df_desc, df_data, group_column, series_column, target_variable, corr_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Selects at most two variables per group based on absolute correlation with the target variable.\n",
    "\n",
    "    Args:\n",
    "      df_desc: DataFrame containing series descriptions and group names.\n",
    "      df_data: DataFrame containing the time series data.\n",
    "      group_column: Name of the column with group labels in df_desc.\n",
    "      series_column: Name of the column with series names in df_desc.\n",
    "      target_variable: The dependent variable for correlation analysis.\n",
    "      corr_threshold: Minimum absolute correlation value for selection.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary with group names as keys and selected variables as values.\n",
    "    \"\"\"\n",
    "    selected_vars = {}\n",
    "    group_names = df_desc[group_column].unique()\n",
    "\n",
    "    for group_name in group_names:\n",
    "        try:\n",
    "            # Get series for the current group\n",
    "            series_names = df_desc[df_desc[group_column] == group_name][series_column]\n",
    "            group_data = df_data[series_names].copy()\n",
    "\n",
    "            # Drop missing values\n",
    "            group_data.dropna(inplace=True)\n",
    "\n",
    "            # Compute correlation with target variable\n",
    "            corr_values = group_data.corrwith(df_data[target_variable]).abs().sort_values(ascending=False)\n",
    "\n",
    "            # Select the top two variables that exceed the correlation threshold\n",
    "            selected_vars[group_name] = list(corr_values[corr_values >= corr_threshold].index[:2])\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Series not found in group {group_name} - {e}\")\n",
    "            selected_vars[group_name] = []\n",
    "\n",
    "    return selected_vars\n",
    "\n",
    "\n",
    "def rfe_feature_selection(df_desc, df_data, group_column, series_column, target_variable):\n",
    "    \"\"\"\n",
    "    Selects at most two variables per group using Recursive Feature Elimination (RFE).\n",
    "\n",
    "    Args:\n",
    "      df_desc: DataFrame containing series descriptions and group names.\n",
    "      df_data: DataFrame containing the time series data.\n",
    "      group_column: Name of the column with group labels in df_desc.\n",
    "      series_column: Name of the column with series names in df_desc.\n",
    "      target_variable: The dependent variable for feature selection.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary with group names as keys and selected variables as values.\n",
    "    \"\"\"\n",
    "    selected_vars = {}\n",
    "    group_names = df_desc[group_column].unique()\n",
    "\n",
    "    for group_name in group_names:\n",
    "        try:\n",
    "            # Get series for the current group\n",
    "            series_names = df_desc[df_desc[group_column] == group_name][series_column]\n",
    "            group_data = df_data[series_names].copy()\n",
    "            group_data.dropna(inplace=True)  # Remove missing values\n",
    "\n",
    "            # Prepare features (X) and target (y)\n",
    "            X = group_data\n",
    "            y = df_data.loc[X.index, target_variable]\n",
    "\n",
    "            # Apply RFE with Linear Regression\n",
    "            model = LinearRegression()\n",
    "            selector = RFE(model, n_features_to_select=min(2, X.shape[1]))  # At most 2 features\n",
    "            selector.fit(X, y)\n",
    "\n",
    "            # Get selected features\n",
    "            selected_vars[group_name] = list(X.columns[selector.support_])\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Series not found in group {group_name} - {e}\")\n",
    "            selected_vars[group_name] = []\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping group {group_name} due to insufficient data: {e}\")\n",
    "            selected_vars[group_name] = []\n",
    "\n",
    "    return selected_vars\n",
    "\n",
    "\n",
    "def mutual_info_feature_selection(df_desc, df_data, group_column, series_column, target_variable):\n",
    "    \"\"\"\n",
    "    Selects at most two variables per group using Mutual Information.\n",
    "\n",
    "    Args:\n",
    "      df_desc: DataFrame containing series descriptions and group names.\n",
    "      df_data: DataFrame containing the time series data.\n",
    "      group_column: Name of the column with group labels in df_desc.\n",
    "      series_column: Name of the column with series names in df_desc.\n",
    "      target_variable: The dependent variable for feature selection.\n",
    "\n",
    "    Returns:\n",
    "      Dictionary with group names as keys and selected variables as values.\n",
    "    \"\"\"\n",
    "    selected_vars = {}\n",
    "    group_names = df_desc[group_column].unique()\n",
    "\n",
    "    for group_name in group_names:\n",
    "        try:\n",
    "            # Get series for the current group\n",
    "            series_names = df_desc[df_desc[group_column] == group_name][series_column]\n",
    "            group_data = df_data[series_names].copy()\n",
    "            group_data.dropna(inplace=True)  # Remove missing values\n",
    "\n",
    "            # Prepare features (X) and target (y)\n",
    "            X = group_data\n",
    "            y = df_data.loc[X.index, target_variable]\n",
    "\n",
    "            # Compute Mutual Information scores\n",
    "            mi_scores = mutual_info_regression(X, y)\n",
    "            mi_df = pd.DataFrame({\"Feature\": X.columns, \"MI_Score\": mi_scores})\n",
    "            mi_df = mi_df.sort_values(\"MI_Score\", ascending=False)\n",
    "\n",
    "            # Select at most two features with the highest MI score\n",
    "            selected_vars[group_name] = list(mi_df[\"Feature\"][:2])\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Series not found in group {group_name} - {e}\")\n",
    "            selected_vars[group_name] = []\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping group {group_name} due to insufficient data: {e}\")\n",
    "            selected_vars[group_name] = []\n",
    "\n",
    "    return selected_vars\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Example Usage\n",
    "# ==============================\n",
    "\n",
    "# Assuming df_FRED_Desc_MD contains the group descriptions\n",
    "# df_FRED_MD contains the time series data\n",
    "# The target variable is 'CPIAUCSL'\n",
    "\n",
    "target_var = \"CPIAUCSL\"\n",
    "\n",
    "# Run Correlation Filter\n",
    "corr_selected = correlation_filter(df_FRED_Desc_MD, df_FRED_MD, \"Group\", \"FRED\", target_var, corr_threshold=0.6)\n",
    "\n",
    "# Run Recursive Feature Elimination\n",
    "rfe_selected = rfe_feature_selection(df_FRED_Desc_MD, df_FRED_MD, \"Group\", \"FRED\", target_var)\n",
    "\n",
    "# Run Mutual Information Selection\n",
    "mi_selected = mutual_info_feature_selection(df_FRED_Desc_MD, df_FRED_MD, \"Group\", \"FRED\", target_var)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- Correlation Filter Selected Variables ---\")\n",
    "for group, variables in corr_selected.items():\n",
    "    print(f\"Group: {group}, Selected: {variables}\")\n",
    "\n",
    "print(\"\\n--- RFE Selected Variables ---\")\n",
    "for group, variables in rfe_selected.items():\n",
    "    print(f\"Group: {group}, Selected: {variables}\")\n",
    "\n",
    "print(\"\\n--- Mutual Information Selected Variables ---\")\n",
    "for group, variables in mi_selected.items():\n",
    "    print(f\"Group: {group}, Selected: {variables}\")\n"
   ],
   "id": "4325cf7d43fd9202"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(df_FRED_MD.head())\n",
    "print(df_FRED_MD.shape)\n",
    "\n",
    "df_FRED_MD_t = df_FRED_MD.copy()\n",
    "df_FRED_MD_t = df_FRED_MD_t[['CPIAUCSL', 'UNRATE', 'HOUST', 'HOUSTNE', 'PERMITNE', 'AAA', 'BAA', 'PERMITMW', 'RETAILx', 'TB3MS', 'TB6MS', 'FEDFUNDS', 'CP3Mx', 'S&P 500']]\n",
    "\n",
    "column_lengths = df_FRED_MD_t.count()\n",
    "print(column_lengths)\n",
    "\n",
    "print(df_FRED_MD_t['S&P 500'])\n"
   ],
   "id": "feadf1366d19d4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "qty_series = df_FRED_MD.shape[1]\n",
    "\n",
    "\n",
    "for i in range(0,qty_series):\n",
    "    str_transf = df_FRED_Desc_MD.iloc[i,2]\n",
    "    str_ticker = df_FRED_Desc_MD.iloc[i,3]\n",
    "    col_ticker = np.where(df_FRED_MD_t.columns == str_ticker)\n",
    "    if len(col_ticker[0]) > 0:\n",
    "        col_ticker = col_ticker[0][0]\n",
    "        df_series = df_FRED_MD_t.iloc[:,col_ticker]\n",
    "        if str_transf == \"First difference of log\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = np.log(df_series).diff()\n",
    "        elif str_transf == \"First difference\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = df_series.diff()\n",
    "        elif str_transf == \"Log\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = np.log(df_series)\n",
    "        elif str_transf == \"Second difference of log\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = np.log(df_series).diff().diff()\n",
    "        elif str_transf == \"Second difference\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = df_series.diff().diff()\n",
    "        elif str_transf == \"First difference of (ratio - 1)\":\n",
    "            df_FRED_MD_t.iloc[:,col_ticker] = df_series.pct_change().diff()\n",
    "\n",
    "df_series = df_FRED_MD['S&P 500']\n",
    "# df_FRED_MD_t['S&P 500'] = np.log(df_series).diff()\n",
    "# df_FRED_MD_t = df_FRED_MD_t.iloc[2:,:] # Removes the first 2 rows due to differencing\n",
    "# print(df_FRED_MD_t.head())\n",
    "# print(df_FRED_MD_t.tail())\n",
    "\n",
    "\n",
    "# print(df_FRED_MD_t.head())\n",
    "# print(df_FRED_MD_t.tail())\n",
    "print(df_FRED_MD_t['S&P 500'])\n",
    "print(df_FRED_MD_t.shape)\n"
   ],
   "id": "733adcb9675c306c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_FRED_MD_t[['CPIAUCSL', 'UNRATE', 'HOUST', 'HOUSTNE', 'AAA', 'BAA', 'RETAILx', 'TB3MS', 'TB6MS', 'FEDFUNDS', 'CP3Mx', 'S&P 500']]\n",
    "df_FRED_MD_t.dropna(inplace=True)\n",
    "\n",
    "print(df_FRED_MD_t['S&P 500'].head())\n",
    "print(df_FRED_MD_t.shape)"
   ],
   "id": "cd0104006e8cfe8a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.dates as mdates\n",
    "\n",
    "def test_stationarity_adf(series):\n",
    "    result = adfuller(series)\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'  {key}: {value}')\n",
    "\n",
    "# Example usage\n",
    "# test_stationarity_adf(df_FRED_MD['S&P 500'])  # Replace 'column_name' with your actual column name\n",
    "# print(df_date)\n",
    "\n",
    "def plot_series(series, df_date=None, title='Time Series'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if df_date is not None:\n",
    "        # plt.plot(df_date, series, label='Time Series')\n",
    "        plt.plot(df_date, series, label='Time Series')\n",
    "        plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
    "        # plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # Customize date format\n",
    "    else:\n",
    "        plt.plot(series, label='Time Series')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date' if df_date is not None else 'Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)  # Rotate x-axis labels by 45 degrees\n",
    "    plt.legend()\n",
    "    plt.tight_layout()       # Adjust layout to prevent clipping\n",
    "    plt.show()\n",
    "\n",
    "def plot_rolling_statistics(series, df_date=None, title='Time Series'):\n",
    "    rolling_mean = series.rolling(window=12).mean()\n",
    "    rolling_std = series.rolling(window=12).std()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(series, label='Original Series')\n",
    "    if df_date is not None:\n",
    "        plt.plot(df_date, series, label='Time Series')\n",
    "        plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "    else:\n",
    "        plt.plot(series, label='Original Series')\n",
    "\n",
    "    plt.plot(rolling_mean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolling_std, color='black', label='Rolling Std')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def difference(series):\n",
    "    return series.diff().dropna()\n",
    "\n",
    "# Example usage\n",
    "# plot_series(df_FRED_MD['S&P 500'], df_date, title='Original S&P 500 Time Series')  # Replace 'column_name' with your actual column name\n",
    "# plot_rolling_statistics(df_FRED_MD['S&P 500'], df_date, title='Original S&P 500 Time Series: Rolling Stats')  # Replace 'column_name' with your actual column name\n",
    "\n",
    "# Difference the series\n",
    "# differenced_series = difference(df_FRED_MD['S&P 500'])\n",
    "\n",
    "# Plot the differenced series\n",
    "params = ['CPIAUCSL', 'UNRATE', 'HOUST', 'AAA', 'BAA', 'RETAILx', 'TB3MS', 'TB6MS', 'FEDFUNDS', 'CP3Mx', 'S&P 500']\n",
    "for i in params:\n",
    "    test_stationarity_adf(df_FRED_MD_t[i])\n",
    "    plot_series(df_FRED_MD_t[i], title=('Differenced Time Series' + i))\n",
    "    # plot_rolling_statistics(df_FRED_MD_t[i], title=('Differenced Time Series: Rolling Stats' + i))\n",
    "\n",
    "\n"
   ],
   "id": "c37181d773c79a16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# Data transformation according to McCraken and Ng (2016)\n",
    "# Monthly database\n",
    "\n",
    "# Normalization (mean = 0 and std = 1)\n",
    "df_FRED_MD_t_norm = pd.DataFrame(data = scale(df_FRED_MD_t),\n",
    "                                 index = df_FRED_MD_t.index,\n",
    "                                 columns = df_FRED_MD_t.columns)\n",
    "df_FRED_MD_t_norm.head()\n",
    "\n",
    "# Data preparation using normalized data\n",
    "\n",
    "# Exclude nan from the transformed time series\n",
    "df_FRED_MD_t_norm_ex_nan = df_FRED_MD_t_norm.dropna()\n",
    "\n",
    "# Dataframe containing inflation time series only\n",
    "# CPIAUCSL - Consumer Price Index for All Urban Consumers: All Items\n",
    "y = df_FRED_MD_t_norm_ex_nan[\"CPIAUCSL\"]\n",
    "X = df_FRED_MD_t_norm_ex_nan.drop(columns = \"CPIAUCSL\")\n",
    "\n",
    "print(\"OTHER DATA\")\n",
    "print(df_FRED_MD_t_norm.head())\n",
    "print(df_FRED_MD_t_norm.tail())\n",
    "\n",
    "# print(y.tail())\n",
    "\n",
    "\n",
    "# scaler_other = StandardScaler()\n",
    "# scaled_all = df_FRED_MD_t.drop(columns = \"CPIAUCSL\")\n",
    "# scaled_all_features = scaler_other.fit_transform(scaled_all)  \n",
    "# df_FRED_MD_t_norm2 = pd.DataFrame(data=scaled_all_features, index=scaled_all.index, columns=scaled_all.columns)\n",
    "# \n",
    "# list = ['CPIAUCSL', 'UNRATE', 'HOUST', 'RETAILx', 'FEDFUNDS', 'CP3Mx', 'S&P 500']\n",
    "# for i in list:\n",
    "#     plot_series(df_FRED_MD_t_norm_ex_nan[i], title=('Differenced Time Series' + i))\n",
    "#     plot_rolling_statistics(df_FRED_MD_t_norm_ex_nan[i], title=('Differenced Time Series: Rolling Stats' + i))\n",
    "\n",
    "\n",
    "# Print the first few rows of both DataFrames to compare \n",
    "# print(\"First few rows using scale method:\\n\", other_data.head()) \n",
    "# print(\"\\nFirst few rows using StandardScaler:\\n\", df_FRED_MD_t_norm2.head())\n",
    "\n",
    "# print(other_data.shape)\n",
    "# print(df_FRED_MD_t_norm_ex_nan.index)"
   ],
   "id": "ec585a471f6afaa8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "index_ref = df_FRED_MD_t_norm_ex_nan.index\n",
    "\n",
    "for rnd_state in range(0,10):\n",
    "    index_train, index_test = train_test_split(index_ref, test_size = 0.2, random_state = rnd_state)\n",
    "\n",
    "    X.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X.csv')\n",
    "    y.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'y.csv')\n",
    "\n",
    "    # Split samples\n",
    "\n",
    "    y_train, y_test = y.loc[index_train], y.loc[index_test]\n",
    "    X_train, X_test = X.loc[index_train], X.loc[index_test]\n",
    "\n",
    "    # Normalization\n",
    "\n",
    "    y_train = pd.DataFrame(data = scale(y_train), index = y_train.index)\n",
    "    y_test = pd.DataFrame(data = scale(y_test), index = y_test.index)\n",
    "\n",
    "    X_train = pd.DataFrame(data = scale(X_train), index = X_train.index, columns = X_train.columns)\n",
    "    X_test = pd.DataFrame(data = scale(X_test), index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "    # Save raw database (split)\n",
    "\n",
    "    y_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'y_train.csv')\n",
    "    y_test.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'y_test.csv')\n",
    "    X_train.to_csv(str_Dir_Plan_PC + str(rnd_state) + ' ' + 'X_train.csv')\n",
    "    X_test.to_csv(str_Dir_Plan_PC+ str(rnd_state) + ' ' + 'X_test.csv')\n",
    "\n",
    "    #####################################################################################################################\n",
    "    #                                                                                                                   #\n",
    "    # Denoising and Compression                                                                                         #\n",
    "    #                                                                                                                   #\n",
    "    #####################################################################################################################\n",
    "\n",
    "    \n",
    "    # Variational autoencoder\n",
    "\n",
    "    X_train_pca, X_test_pca = pca_decomp(X_train, X_test, threshold = 0.9, plot_name = 'X_full_pca')\n",
    "\n",
    "    # Reads dimensions from PCA\n",
    "    n = X_train.shape[1]\n",
    "    print(\"N\", n)\n",
    "\n",
    "    n_pca = X_train_pca.shape[1]\n",
    "    print(\"N_PCA\", n_pca)\n",
    "\n",
    "    X_train_vae, X_test_vae = vae(X_train, X_test,\n",
    "                                  intermediate_dim = int((n + n_pca)/2), latent_dim = n_pca,\n",
    "                                  batch_size = 16, epochs = 100,\n",
    "                                  verbose = False, plot_name = 'X_full_vae')\n",
    "\n",
    "    X_train_ae, X_test_ae = deep_ae(X_train, X_test,\n",
    "                                    intermediate_dim = int((n + n_pca)/2), latent_dim = n_pca,\n",
    "                                    batch_size = 16, epochs = 100,\n",
    "                                    verbose = False, plot_name = 'X_full_ae')\n",
    "    \n",
    "\n",
    "    \n"
   ],
   "id": "d3dacbf80020516a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
